{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnijhuis-dnb/Artificial_Intelligence_and_Machine_Learning_for_SupTech/blob/main/Tutorials/Tutorial%203%20Data%20pre-processing%20and%20assessing%20model%20performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x9uRovz0R8V"
      },
      "source": [
        "## Artificial Intelligence and Machine Learning for SupTech  \n",
        "Tutorial 3: Data pre-processing and assessing model performance\n",
        "\n",
        "*\tHow to pre-process: standardize your data\n",
        "*\tPros and cons of standardization\n",
        "*\tWorking with the confusion matrix\n",
        "  *\tWhat if costs are not symmetric?\n",
        "  *\tThe trade-off between precision and recall\n",
        "\n",
        "\n",
        "<br/>\n",
        "\n",
        "14 March 2023  \n",
        "\n",
        "**Instructors**  \n",
        "Prof. Iman van Lelyveld (iman.van.lelyveld@vu.nl)<br/>\n",
        "Dr. Michiel Nijhuis (m.nijhuis@dnb.nl)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Whl7phFpqJR8"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fic-DPeTKChS"
      },
      "source": [
        "### Previous Tutorials\n",
        "In this section we re-run some of the code from the first 2 tutorials to have a starting model. With these steps out of the way we can focus on the pre-processing and the further evaluating of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xq0uEXnK1erY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFTwgFfw1taU"
      },
      "outputs": [],
      "source": [
        "!gdown 1-3c9BhPfl6D92HvTI4kNd0MfmTquiUwQ\n",
        "!gdown 1-5ZzK3EAqc-i3AgnLOSZXTGGZsEPEmzH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpxP2w-isyjq"
      },
      "outputs": [],
      "source": [
        "path = 'credit_record.csv'\n",
        "df_record = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-Gmuwo-syjr"
      },
      "outputs": [],
      "source": [
        "path = 'application_record.csv'\n",
        "df_applications = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_0sf0Jjsyjr"
      },
      "outputs": [],
      "source": [
        "df_record.loc[:,'status'] = df_record.loc[:,'STATUS']\n",
        "df_record.loc[:,'status'] = df_record.loc[:,'status'].replace('X', '0')\n",
        "df_record.loc[:,'status'] = df_record.loc[:,'status'].replace('C', '0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px6-q5Qysyjs"
      },
      "outputs": [],
      "source": [
        "df_record.loc[:,'status'] = pd.to_numeric(df_record.loc[:,'status'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux_-BJy9syjt"
      },
      "outputs": [],
      "source": [
        "sr_defaults = df_record.groupby('ID')['status'].agg(lambda x: sum(x>=2)>0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yq6ZURwdsyju"
      },
      "outputs": [],
      "source": [
        "df_applications = df_applications.drop_duplicates(subset='ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoSRszIqsyjv"
      },
      "outputs": [],
      "source": [
        "df_applications = df_applications.set_index('ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovTcseW0syjw"
      },
      "outputs": [],
      "source": [
        "df_applications = df_applications.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdixnHsgVKCA"
      },
      "outputs": [],
      "source": [
        "obj_cols = df_applications.select_dtypes(include=['object']).columns.tolist()\n",
        "dummies_list = [pd.get_dummies(df_applications[col], prefix=col, drop_first=True) for col in obj_cols]\n",
        "df_applications = pd.concat([df_applications.drop(columns=obj_cols)] + dummies_list, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50Uo0hr-VbJB"
      },
      "outputs": [],
      "source": [
        "df_data = df_applications.merge(sr_defaults, how='inner', left_index=True, right_on='ID')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCZHB6vhWPqx"
      },
      "outputs": [],
      "source": [
        "df_data= df_data.rename(columns={'status':'DEFAULTED'}).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAHQJ8OBIUmu"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaQk3jW9syj0"
      },
      "outputs": [],
      "source": [
        "clf = SVC(C=1.0, \n",
        "          kernel='rbf', \n",
        "          degree=3, \n",
        "          gamma='scale', \n",
        "          coef0=0.0, \n",
        "          shrinking=True, \n",
        "          probability=False, \n",
        "          tol=0.1, \n",
        "          cache_size=200, \n",
        "          class_weight=None, \n",
        "          verbose=False, \n",
        "          max_iter=5, \n",
        "          decision_function_shape='ovr', \n",
        "          break_ties=False, \n",
        "          random_state=43)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXpK31fGvehc"
      },
      "outputs": [],
      "source": [
        "X = df_data.drop(columns='DEFAULTED')\n",
        "y = df_data['DEFAULTED']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vIJ9t28Iu54"
      },
      "outputs": [],
      "source": [
        "clf = clf.fit(X.iloc[:10000], y.iloc[:10000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lO2w1k8dNHr-"
      },
      "outputs": [],
      "source": [
        "y_model = clf.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOF1F3xG13Le"
      },
      "source": [
        "### Tutorial 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2M07_-fwI3p"
      },
      "source": [
        "Evaluate the performance based on the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7QFZcJHMwvh"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJc1MsjJwIMB"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y, y_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngHtx430_mMw"
      },
      "source": [
        "Instead of the predictions we can also get the scores the SVM produces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb6KpjOj_tsG"
      },
      "outputs": [],
      "source": [
        "clf.predict_proba(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrwjVw9N_3aa"
      },
      "source": [
        "What was the original prediction threshold?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0gJmK6H_-xq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuUyQT7H6Ir5"
      },
      "source": [
        "Imagine we value the recall twice as much as the precision, can you adjust the decision threshold to get an optimum prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kny88Zw5_eN"
      },
      "source": [
        "An easier way to evaluate the performance is with the precision recall curve. We can use the following function for that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeRBIJX8588C"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_recall_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zedTMGWhBLLO"
      },
      "source": [
        "Can you make a precision recall curve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LM_e7CFN6Typ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en0dcuk_yJdL"
      },
      "source": [
        "Let's have a look at the distribution of one of the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ChBJMq3P50f"
      },
      "outputs": [],
      "source": [
        "df_data['DAYS_EMPLOYED'].plot.hist(bins=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8TXw1rpzUtl"
      },
      "source": [
        "As you can see the distribution of the data is not that ideal for most machine learning algorithms. Can you improve the prediction of the DEFAULT rate, by adjusting the parameters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pvcmdx_A416y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA0ZCMw_6VPg"
      },
      "source": [
        "Another point with the DAYS_EMPLOYED variable is that it's range is between -16000 and 0, this is much higher than most other variables, can you scale this and other variables to a better range?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orOKr45d6kIq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRTeE2EZBmbL"
      },
      "source": [
        "Does this lead to a better model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoXzeX2LBn0i"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
