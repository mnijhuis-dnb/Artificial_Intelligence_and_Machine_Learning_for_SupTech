{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mnijhuis-dnb/Artificial_Intelligence_and_Machine_Learning_for_SupTech/blob/main/Tutorials/Tutorial%207%20NLTK%20and%20sentiment%20analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x9uRovz0R8V"
      },
      "source": [
        "## Artificial Intelligence and Machine Learning for SupTech  \n",
        "Tutorial 7: NLTK and sentiment analysis\n",
        "\n",
        "*\tConstructing a bag of words\n",
        "*\tClassifying sentiments (positive/negative)\n",
        "*\tExample with financial news data\n",
        "\n",
        "<br/>\n",
        "\n",
        "15 March 2023  \n",
        "\n",
        "**Instructors**  \n",
        "Prof. Iman van Lelyveld (iman.van.lelyveld@vu.nl)<br/>\n",
        "Dr. Michiel Nijhuis (m.nijhuis@dnb.nl)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1b1zu7x3drW"
      },
      "source": [
        "### Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAUm1PTv3Y5I"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "for the natural language processing we need to download some word sets to be able to use some of the methods within the nltk package  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4CjVU8TPOWo"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('vader_lexicon')\n",
        "!pip install twython "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frYw4kD24Jyc"
      },
      "source": [
        "Load articles from the FT. The datafile contains 436 articles from the Financial Times on the topic of Environmental, Social and Governance (ESG) between March 2021 and 2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOEtgav4_-bS"
      },
      "outputs": [],
      "source": [
        "!gdown 1-4lW68b60PikNWQOfqc5qtriOEHC3_Kq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC7TNg_V4Jyd"
      },
      "outputs": [],
      "source": [
        "df_articles = pd.read_csv('/content/df_articles.csv')\n",
        "df_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW_ZrPJI4Jyd"
      },
      "source": [
        "### Extract tokens from the text\n",
        "Current, the article is stored as `date`, `title` and `body`. Our focus will be on `body`. This is currently stored as a long string of text. See example below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v-xDoJiKKiz"
      },
      "outputs": [],
      "source": [
        "df_articles.loc[10, 'title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5sQTRWd4Jyd"
      },
      "outputs": [],
      "source": [
        "df_articles.loc[10, 'body']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBZO9zSe4Jyd"
      },
      "source": [
        "This is not particularly useful as we want to extract meaningful words. For example, \"green\" or \"inflation\" or \"rise\". For this we first have to splice the `body` into individual tokens, that are separated by white spaces. The regular expression `'\\w+'` means one or more alphanumeric characters. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqJp_KMP4Jyd"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "# \\w+ means at least one white space or line break character\n",
        "regexp = RegexpTokenizer('\\w+')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjLNfKpK4Jyd"
      },
      "outputs": [],
      "source": [
        "df_articles['body_tokenize'] = df_articles['body'].apply(regexp.tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGLnrmYrLg0h"
      },
      "outputs": [],
      "source": [
        "df_articles.loc[0, 'body']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nwfu12jdg7h"
      },
      "outputs": [],
      "source": [
        "df_articles.loc[0, 'body_tokenize']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HdKZbaa6G7D"
      },
      "source": [
        "Convert all words to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWK0EGO76Nb6"
      },
      "outputs": [],
      "source": [
        "def convert_to_lowercase(list_of_tokens):\n",
        "  return [tk.lower() for tk in list_of_tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNVFvuLjd8oe"
      },
      "outputs": [],
      "source": [
        "convert_to_lowercase(['BaBaBa','bbbb','AAAA'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llJsqUkz6J-i"
      },
      "outputs": [],
      "source": [
        "df_articles['body_tokenize'] = df_articles['body_tokenize'].apply(convert_to_lowercase)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jZ7vl5k6wbB"
      },
      "outputs": [],
      "source": [
        "df_articles.loc[10, 'body_tokenize']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwAELygp4Jye"
      },
      "source": [
        "This is much better. But many words are not informative, which are what we call \"stopwords\". We can now remove stopwords. To be able to do this we should make use of a certain corpus. To view the nltk stopword corpus we can use the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-64eOIwn4Jye"
      },
      "outputs": [],
      "source": [
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW4g8XfR4Jye"
      },
      "source": [
        "We want to filter out these words and only keep the meaningful ones. Moreover, short words that are one or two characters long can also be removed, just like words containing just numbers. Can you write a function that takes in a list of strings and removes the stopwords and returns the list without the stopwords?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens):\n",
        "    non_stopwords_tokens = []\n",
        "    for tk in tokens:\n",
        "        if len(tk) < 3:\n",
        "            # if the token is less than 3 characters long, jump to next token\n",
        "            continue\n",
        "\n",
        "        if any([a.isnumeric() for a in tk]):\n",
        "            # if the token is numeric, e.g. '34' yields true\n",
        "            continue\n",
        "\n",
        "        if tk in stopwords:\n",
        "            # if the token is a stopword, jump to next token\n",
        "            continue\n",
        "\n",
        "        # if no jumps happened, then add the token to the results\n",
        "        non_stopwords_tokens.append(tk)\n",
        "\n",
        "    return non_stopwords_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To test the function you can use the following code to test it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtZDEhO9egjV"
      },
      "outputs": [],
      "source": [
        "remove_stopwords([\n",
        "    'ab',\n",
        "    'abc',\n",
        "    'only',\n",
        "    '2020a',\n",
        "    'England',\n",
        "    'ESG',\n",
        "    'of',\n",
        "    'aaaaaaa'\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did you remove all of the stopwords correctly? The following function would also remove these words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmY-cMPW4Jye"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens):\n",
        "    non_stopwords_tokens = []\n",
        "    for tk in tokens:\n",
        "        if len(tk) < 3:\n",
        "            # if the token is less than 3 characters long, jump to next token\n",
        "            continue\n",
        "\n",
        "        if any([a.isnumeric() for a in tk]):\n",
        "            # if the token is numeric, e.g. '34' yields true\n",
        "            continue\n",
        "\n",
        "        if tk in stopwords:\n",
        "            # if the token is a stopword, jump to next token\n",
        "            continue\n",
        "\n",
        "        # if no jumps happened, then add the token to the results\n",
        "        non_stopwords_tokens.append(tk)\n",
        "\n",
        "    return non_stopwords_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsTQgI3m4Jye"
      },
      "outputs": [],
      "source": [
        "df_articles['body_tokenize_nonstop'] = df_articles['body_tokenize'].apply(remove_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv4pwzYZ4Jye"
      },
      "outputs": [],
      "source": [
        "df_articles.loc[10, 'body_tokenize_nonstop']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nx6RfcTB4Jye"
      },
      "source": [
        "### Stemming and lemmatization\n",
        "\n",
        "Great, now we need to use stemming or lemmatization to identify inflected forms of a word. I.e. `look` is the base form, but we usually see words like `looking`, `looks`, `looked` etc. Lemmatization goes a step further and also takes the context into account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AZbA_Of4Jye"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer().lemmatize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrdjVa6G4Jye"
      },
      "outputs": [],
      "source": [
        "lemmatizer('bikes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ss_cXFc54Jye"
      },
      "outputs": [],
      "source": [
        "lemmatizer('going')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can you write a function to lemmatize the words within a list of words? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def lemmatize_each_word(words):\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "        lemmatized = lemmatizer(word)\n",
        "        lemmatized_words.append(lemmatized)\n",
        "    return lemmatized_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To test the lemmatization you can use the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lemmatize_each_word([\n",
        "    'is',\n",
        "    'the',\n",
        "    'code',\n",
        "    'working',\n",
        "    'well',\n",
        "    'or',\n",
        "    'does',\n",
        "    'it',\n",
        "    'not',\n",
        "    'work'\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Did it go correctly, otherwise you can use the following function to lemmatize the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVoN7Lz04Jyf"
      },
      "outputs": [],
      "source": [
        "def lemmatize_each_word(words):\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "        lemmatized = lemmatizer(word)\n",
        "        lemmatized_words.append(lemmatized)\n",
        "    return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8JzN6FE4Jyf"
      },
      "outputs": [],
      "source": [
        "lemmatize_each_word(df_articles.loc[10, 'body_tokenize_nonstop'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6oT2nZP4Jyf"
      },
      "outputs": [],
      "source": [
        "df_articles['body_tokenize_nonstop_lemma'] = df_articles['body_tokenize_nonstop'].apply(lemmatize_each_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0OIEJ7X4wwQ"
      },
      "source": [
        "Now we can assign sentiments to the articles we have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J1DR_-JE4Jyf"
      },
      "outputs": [],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer().polarity_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neU1Sqb3GywW"
      },
      "outputs": [],
      "source": [
        "analyzer('good')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWGLCgstG7qv"
      },
      "outputs": [],
      "source": [
        "analyzer('bad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG3OPSSeG9CV"
      },
      "outputs": [],
      "source": [
        "analyzer('I like coffee')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the sentiment we can also write a function to get the sentiment from a list of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDzCXDiK4Jyf"
      },
      "outputs": [],
      "source": [
        "def get_sentiment(list_of_words):\n",
        "    words = ' '.join(list_of_words)\n",
        "    analyzed = analyzer(words)\n",
        "    return analyzed['compound']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QjRKEV6HBvM"
      },
      "outputs": [],
      "source": [
        "get_sentiment(['I', 'like', 'coffee'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO9Vixes4Jyf"
      },
      "outputs": [],
      "source": [
        "df_articles['sentiment'] = df_articles['body_tokenize_nonstop_lemma'].apply(get_sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PQfCiqvHNkj"
      },
      "outputs": [],
      "source": [
        "df_articles['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5qQcMFXHSoz"
      },
      "outputs": [],
      "source": [
        "df_articles.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysVYsQtsHdoI"
      },
      "source": [
        "We now have the desired results. We would like to plot these over time. So let's extract the `sentiment` column and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6_pcyMPHkmi"
      },
      "outputs": [],
      "source": [
        "df_articles['sentiment'].plot(figsize=[15,5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqKvVhx8Ho1S"
      },
      "source": [
        "This is very messy. There are several issues here that need to be addressed\n",
        "1. The x-axis is wrong, it should be based on dates, not number of entries.\n",
        "2. The series is very volatile, we are looking for a trend.\n",
        "\n",
        "Let us first make our problem easier. `df_articles` contains all the data we constructed throughout the notebook. For plotting, however, we only need two columns: (a) the `date` column as the index and (b) the `sentiment` column for the actual time series values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBhq4aHrIS6n"
      },
      "outputs": [],
      "source": [
        "df_articles.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN-nAbqhMXPW"
      },
      "source": [
        "Setting the dates correct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSfarYwgI8At"
      },
      "outputs": [],
      "source": [
        "sr_sentiment = df_articles.set_index('date')\n",
        "sr_sentiment = sr_sentiment['sentiment']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0NUpl_ZJFWR"
      },
      "outputs": [],
      "source": [
        "sr_sentiment.plot(figsize=[15,5], marker='o')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRy-9bWIJ2_h"
      },
      "source": [
        "This looks identical as the previous figure, only that the x-labels are different. If we look closely, however, the dates are not actual dates. Instead, these are texts. Let's have a look at the index of `sr_sentiment`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zKxwGIVKIF3"
      },
      "outputs": [],
      "source": [
        "sr_sentiment.index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwxNln4zKKiP"
      },
      "source": [
        "The `dtype='object'` means that the index is just a list of string values. We want to have dates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUF0pr-pKRGv"
      },
      "outputs": [],
      "source": [
        "sr_sentiment.index = pd.to_datetime(sr_sentiment.index)\n",
        "sr_sentiment.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGIxgjEgKXWu"
      },
      "outputs": [],
      "source": [
        "sr_sentiment.plot(figsize=[15,5], marker='o', lw=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D3K6VPbJUnD"
      },
      "source": [
        "What's different? A lot of diagonal lines appear that seem to jump between data points. This is usually a sign of interpolation and missing values. \n",
        "\n",
        "Can we see the missing values? Let's have look at `sr_sentiment`. Notice anything strange?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65nexg8NKkj1"
      },
      "outputs": [],
      "source": [
        "sr_sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can you make a nicer graph of how the sentiment varies over time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sr_sentiment.groupby([sr_sentiment.year, sr_sentiment.week]).mean().plot(figsize=[15,5])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
